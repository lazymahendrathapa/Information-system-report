
\documentclass[conference]{IEEEtran}
\usepackage{blindtext, graphicx}
\usepackage[super,square]{natbib}



\begin{document}
\title{Scalability of Hadoop Framework}

\author{\IEEEauthorblockN{Ayush Shakya}
\IEEEauthorblockA{Deparment of Electronics and\\Computer Engineering\\
Institute of Engineering,\\
Pulchowk, Lalitpur\\
Email: }
\and
\IEEEauthorblockN{Bijay Gurung}
\IEEEauthorblockA{Deparment of Electronics and\\Computer Engineering\\
Institute of Engineering,\\
Pulchowk, Lalitpur\\
Email:}
\and
\IEEEauthorblockN{Mahendra Singh Thapa}
\IEEEauthorblockA{Deparment of Electronics and\\Computer Engineering\\
Institute of Engineering,\\
Pulchowk, Lalitpur\\
Email:}
\and
\IEEEauthorblockN{Mehang Rai}
\IEEEauthorblockA{Deparment of Electronics and\\Computer Engineering\\
Institute of Engineering,\\
Pulchowk, Lalitpur\\
Email: mehang.rai007@gmail.com}
}

\maketitle
\begin{abstract}
        Written at last.
\end{abstract}
% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
        Hadoop, scalability, distributed system, MapReduce.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle



\section{Introduction}
Apache Hadoop is an open-source software framework for distributed storage
and distributed processing of very large data sets on computer clusters built from commodity
hardware.\cite{Apache} Hadoop is an Apache top-level project being built and used by a global
community of contributors and users. It is licensed under the Apache License 2.0.\cite{Introapahe} 
Hadoop was created by Doug Cutting and Mike Cafarella in 2005.
\par The Hadoop framework is mostly written in Java programming language,
with some native code in C programming language and command line utilities
written as shell scripts. Though MapReduce Java code is common, any programming language can
be used with “Hadoop Streaming” to implement the “map” and “reduce” parts of the user’s program.\cite{Hadoopadventures}
Other projects in the Hadoop ecosystem expose richer user interfaces.
All the modules in Hadoop are designed with a fundamental assumption that
hardware failures are common and should be automatically handled by the framework.
It provides massive storage for any kind of data, enormous processing power and the ability to handle virtually
limitless concurrent tasks or jobs.
\par Some key features of Hadoop are:
\begin{enumerate}
        \item \textbf{Ability to store and process huge amounts of any kind of data quickly:}
                With data volumes and varieties constantly increasing, especially from social media and the Internet of Things(IoT), that’s a key consideration.
        \item \textbf{Computing power:} Hadoop’s distributed computing model processes big data fast. The more computing nodes you use, the more processing power you have.
        \item \textbf{Fault tolerance:} Data and application processing are protected against hardware failure. If a node goes down, jobs are automatically redirected to other nodes to make sure the distributed computing does not fail. Multiple copies of all data are stored automatically.
        \item \textbf{Flexibility:} Unlike traditional relational databases, you don’t have to preprocess data before storing it. You can store as much data as you want and decide how to use it later. That includes unstructured data like text, images and videos.
        \item \textbf{Low cost:} The open-source framework is free and uses commodity hardware to store large quantities of data.
        \item \textbf{Scalability:} You can easily grow your system to handle more data simply by adding nodes. Little administration is required.
\end{enumerate}
\par Since unstructured data are quite complex and voluminous as they come from a variety
of sources, such as emails, text documents, videos, photos, audio files,etc., traditional
database cannot handle them. In such case, hadoop’s ability to join, aggregate, and analyze
vast stores of multi-source data without having to structure it first allows organizations to
gain deeper insights quickly. But hadoop must not be confused as a database. You can easily grow your system to handle more data simply by adding nodes. Little administration is required.
\par Hadoop makes it possible to run applicaitons on systems with thousands of nodes involving thousands of terabytes. It's distributed file system facilities rapid
data transfer rates among nodes and allows the system to continue operating uninterrupted in case of a node failure. This approach lowers the risk of catastrphic system failure,
even if a significant number of nodes become inoperative.

\section{History of Hadoop}
As the World Wide Web grew in the late 1900s and early 2000s, search engines and indexes were created to help locate relevant information amid the text-based content. In the early years, search results were returned by humans. But as the web grew from dozens to millions of pages, automation was needed. Web crawlers were created, many as university-led research projects, and search engine start-ups took off (Yahoo, AltaVista, etc.).
\par One such project was an open-source web search engine called Nutch – the brainchild of Doug Cutting and Mike Cafarella. They wanted to return web search results faster by distributing data and calculations across different computers so multiple tasks could be accomplished simultaneously. During this time, another search engine project called Google was in progress. It was based on the same concept – storing and processing data in a distributed, automated way so that relevant web search results could be returned faster.
\par In 2006, Cutting joined Yahoo and took with him the Nutch project as well as ideas based on Google’s early work with automating distributed data storage and processing which was published in October 2003. This paper spawned another research paper from Google - MapReduce: Simplified Data Processing on Large Clusters in December, 2004. The Nutch project was divided – the web crawler portion remained as Nutch and the distributed computing and processing portion became Hadoop (named after Cutting’s son’s yellow plush toy elephant). In 2008, Yahoo released Hadoop as an open-source project. The initial code that was factored out of Nutch consisted of 5k lines of code for NDFS and 6k lines of code for MapReduce. Today, Hadoop’s framework and ecosystem of technologies are managed and maintained by the non-profit Apache Software Foundation (ASF), a global community of software developers and contributors.
\par The first committer added to the Hadoop project was Owen O’Malley in March 2006. Hadoop 0.1.0 was released in April 2006 and continues to evolve by the many contributors to the Apache Hadoop project.
\par Yahoo started using Hadoop on a 1000 node cluster. Then on january 2008, Apache took over Hadoop. In July 2008, 4000 node cluster were tested successfully with Hadoop. Hadoop successfully sorted a petabyte
of data in less than 17 hours to handle billions of searches and indexing millions of web pages.
Until this date, several version of Hadoop has been released. The latest version of Hadoop is version 2.7 which was released on June 2015.
\par Since Hadoop is an open-source so many popular company has contributed to it. With the likes of Google and Yahoo being already involved in it's early
development, there are popular companies like Facebook, eBay, etc. are contributing to it. On January 2011, Facebook, LinkedIn, eBay and IBM collectively contributed 200,000 lines of code. By June 2011, Yahoo had 
42k Hadoop nodes and hundreds of petabytes of storage.\cite{Wikiapache}
\par Cloudera was founded in 2008 to commercialize Apache Hadoop, with an awareness of three key trends that are changing the way the entire industry stores and uses data. These changes came first to consumer Internet properties, but loom large or commercial, defense, and
intelligence applicaitons as well.

\section{Overview of Technologies}
The core of Apache Hadoop consists of a storage part, known as Hadoop Distributed File System (HDFS), and a processing part called MapReduce. Hadoop splits files into large blocks and distributes them across nodes in cluster. To process data, Hadoop transfers packaged code for nodes to process in parallel
based on the data that needs to be processed. This approach takes advantage of data locality - nodes manipulating tha data they have access to - to allow the dataset to be processes faster and more efficiently than it would be in a more conventional supercomputer architecture that relies on a parallel file 
system where computation and data are distributed via high-speed networking.
\par The base Apache Hadoop framework is composed of the following modules:
\begin{enumerate}
        \item \textbf{Hadoop Common:}
                It contains libraries and utilities needed by other Hadoop modules.
        \item \textbf{Hadoop Distributed File System (HDFS):}
It is a distributed file-system that stores data on commodity machines, providing very high aggregate bandwidth across the cluster.
It is one of the primary components at the core of Apache Hadoop 1.x. It is an open source projects, inspired by technologies created inside Google.
\begin{figure}[h!]
        \centering
        \includegraphics[width=80mm]{hdfs}
        \caption{Architecture of Hadoop distributed file system}
\end{figure}
The Hadoop distributed file system (HDFS) is a distributed, scalable, and portable file-system written in 
Java for the Hadoop framework. Each node in a Hadoop instance typically has a single namenode, and a cluster
of datanodes form the HDFS cluster. The situation is typical because each node does not require a datanode
to be present. Each datanode serves up blocks of data over the network using a block protocol specific to
HDFS. The file system uses the TCP/IP layer for communication. Clients use Remote procedure call (RPC) to
communicate between each other.\\
\par HDFS stores large files (typically in the range of gigabytes to terabytes) across multiple machines. It achieves reliability by replicating the data across multiple hosts, and hence does not require RAID storage on hosts. With the default replication value, 3, data is stored on three nodes: two on the same rack, and one on a different rack. Data nodes can talk to each other to rebalance data, to move copies around, and to keep the replication of data high. HDFS is not fully POSIX-compliant, because the requirements for a POSIX file-system differ from the target goals for a Hadoop application. The tradeoff of not having a fully POSIX-compliant file-system is increased performance for data throughput and support for non-POSIX operations such as Append.
\par The HDFS file system includes a so-called secondary namenode, which misleads some people into thinking that when the primary namenode goes offline, the secondary namenode takes over. In fact, the secondary namenode regularly connects with the primary namenode and builds snapshots of the primary namenode's directory information, which the system then saves to local or remote directories. These checkpointed images can be used to restart a failed primary namenode without having to replay the entire journal of file-system actions, then to edit the log to create an up-to-date directory structure. Because the namenode is the single point for storage and management of metadata, it can become a bottleneck for supporting a huge number of files, especially a large number of small files. HDFS Federation, a new addition, aims to tackle this problem to a certain extent by allowing multiple name-spaces served by separate namenodes.
\par An advantage of using HDFS is data awareness between the job tracker and task tracker. The job tracker schedules map or reduce jobs to task trackers with an awareness of the data location.
This reduces the amount of traffic that goes over the network and prevents unnecessary data transfer. When Hadoop is used with other file systems, this advantage is not always available. This can have a significant impact on job-completion times, which has been demonstrated when running data-intensive jobs. HDFS was designed for mostly immutable files and may not be suitable for systems requiring concurrent write-operations.
\par Another limitation of HDFS is that it cannot be mounted directly by an existing operating system. Getting data into and out of the HDFS file system, an action that often needs to be performed before and after executing a job, can be inconvenient. A filesystem in Userspace (FUSE) virtual file system has been developed to address this problem, at least for Linux and some other Unix systems.
\par File access can be achieved through the native Java API, the Thrift API, to generate a client in the language of the users' choosing (C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C\#, Cocoa, Smalltalk, or OCaml), the command-line interface, or browsed through the HDFS-UI web app over HTTP.

        \item \textbf{Hadoop MapReduce:}
                It is an implementation of the MapReduce programming model for large scale data processing
It is one of the primary components at the core of Apache Hadoop 1.x. It is an open source projects, inspired by technologies created inside Google.
\begin{figure}[h!]
        \centering
        \includegraphics[width=80mm]{mapreduce}
        \caption{Architecture of Hadoop distributed file system}
\end{figure}
With Hadoop distributed file system comes the MapReduce engine, which consists of one JobTracker, to which client applications submit MapReduce jobs.
The JobTracker pushes work out to available TaskTracker nodes in the cluster, striving to keep the work as close to the data as possible.
\par With a rack-aware file system, the JobTracker knows which node contains the data, and which other machines are nearby. If the work cannot be hosted on the actual node where the data resides, priority is given to nodes in the same rack. This reduces network traffic on the main backbone network.
\par If a TaskTracker fails or times out, that part of the job is rescheduled. The TaskTracker on each node spawns off a separate Java Virtual Machine process to prevent the TaskTracker itself from failing if the running job crashes the JVM. A heartbeat is sent from the TaskTracker to the JobTracker every few minutes to check its status. The Job Tracker and TaskTracker status and information is exposed by Jetty and can be viewed from a web browser.
\par If the JobTracker failed on Hadoop 0.20 or earlier, all ongoing work was lost. Hadoop version 0.21 added some checkpointing to this process. The JobTracker records what it is up to in the file system. When a JobTracker starts up, it looks for any such data, so that it can restart work from where it left off.
\par MapReduce programming is not a good match for all problems. It’s good for simple information requests and problems that can be divided into independent units, but it's not efficient for iterative and interactive analytic tasks. MapReduce is file-intensive. Because the nodes don’t intercommunicate except through sorts and shuffles, iterative algorithms require multiple map-shuffle/sort-reduce phases to complete. This creates multiple files between MapReduce phases and is inefficient for advanced analytic computing.

        \item \textbf{Hadoop YARN:}
\end{enumerate}



\subsection{Subsection Heading Here}
\blindtext


%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


%
\section{Conclusion}
\blindtext





\appendices
\section{Proof of the First Zonklar Equation}
\blindtext

% use section* for acknowledgement
\section*{Acknowledgment}


The authors would like to thank...


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



\begin{thebibliography}{1}

\bibitem{Apache}
        \emph{Welcome to Apache Hadoop.}
        hadoop.apache.org.
        January 31, 2015.
\bibitem{Introapache}
        \emph{An introduction to Apache Hadoop.}
        https://opensource.com/life/14/8/intro-apache-hadoop-big-data.
\bibitem{Hadoopadventures}
        \emph{Adventures with Hadoop and Perl.}
        Mail-archive.com.
        May 2, 2010.
\bibitem{Wikiapache}
        \emph{Apache Hadoop.}
        https://en.wikipedia.org/wiki/Apache\_Hadoop.
\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{picture}}]{John Doe}
\blindtext
\end{IEEEbiography}

\end{document}


